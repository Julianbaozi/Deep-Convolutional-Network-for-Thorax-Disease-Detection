{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from baseline_cnn import *\n",
    "\n",
    "import numpy as np\n",
    "import PIL\n",
    "from metrics import evaluate\n",
    "from mybceloss import MyBCEloss\n",
    "import torchvision.models as models\n",
    "from xray_dataloader_z_score import ChestXrayDataset, create_split_loaders\n",
    "import torchvision\n",
    "from torchvision import transforms, utils\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "import torch.nn.init as torch_init\n",
    "import torch.optim as optim\n",
    "\n",
    "# Setup: initialize the hyperparameters/variables\n",
    "num_epochs = 1           # Number of full passes through the dataset\n",
    "batch_size = 16          # Number of samples in each minibatch\n",
    "learning_rate = 0.001  \n",
    "seed = np.random.seed(1) # Seed the random number generator for reproducibility\n",
    "p_val = 0.1              # Percent of the overall dataset to reserve for validation\n",
    "p_test = 0.2             # Percent of the overall dataset to reserve for testing\n",
    "\n",
    "def test(model,dataset):\n",
    "    with torch.no_grad():\n",
    "        temp_loss = 0\n",
    "        temp_acc = 0\n",
    "        temp_precision=0\n",
    "        temp_recall = 0\n",
    "        temp_BCR =0\n",
    "        for minibatch_count,(images,labels) in enumerate(dataset,0):\n",
    "            images,labels = images.to(computing_device),labels.to(computing_device)\n",
    "            outputs = model(images)\n",
    "            outputs = torch.sigmoid(outputs) \n",
    "            loss = criterion(outputs,labels)\n",
    "            (acc,pre,rec,BCR),_ = evaluate(outputs.cpu().data.numpy(),label = labels.cpu().data.numpy())\n",
    "            temp_loss += loss\n",
    "            temp_acc +=acc\n",
    "            temp_precision+=pre\n",
    "            temp_recall +=rec\n",
    "            temp_BCR +=BCR\n",
    "\n",
    "        temp_loss= temp_loss/(minibatch_count+1)\n",
    "        temp_acc= temp_acc/(minibatch_count+1)\n",
    "        temp_precision= temp_precision/(minibatch_count+1)\n",
    "        temp_recall= temp_recall/(minibatch_count+1)\n",
    "        temp_BCR= temp_BCR/(minibatch_count+1)\n",
    "        print(\"Validation loss after %d minibatch is %.3f,acc is %.3f,precision is %.3f,recall is %.3f,BCR is %.3f\"%(minibatch_count,temp_loss,temp_acc,temp_precision,temp_recall,temp_BCR))\n",
    "        \n",
    "        return(temp_loss)\n",
    "        \n",
    "\n",
    "\n",
    "#TODO: Convert to Tensor - you can later add other transformations, such as Scaling here\n",
    "transform = transforms.Compose([\n",
    "                                transforms.Resize((224,224)),\n",
    "                                transforms.RandomRotation(20, resample=PIL.Image.BILINEAR),\n",
    "                                transforms.ColorJitter(brightness=64/255, contrast=.25, saturation=.25, hue=.04), \n",
    "                                #http://www.voidcn.com/article/p-dmjhonsq-bgn.html\n",
    "                                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                transforms.ToTensor()])\n",
    "\n",
    "\n",
    "# Check if your system supports CUDA\n",
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "# Setup GPU optimization if CUDA is supported\n",
    "if use_cuda:\n",
    "    computing_device = torch.device(\"cuda\")\n",
    "    extras = {\"num_workers\": 4, \"pin_memory\": True}\n",
    "    print(\"CUDA is supported\")\n",
    "else: # Otherwise, train on the CPU\n",
    "    computing_device = torch.device(\"cpu\")\n",
    "    extras = False\n",
    "    print(\"CUDA NOT supported\")\n",
    "\n",
    "# Setup the training, validation, and testing dataloaders\n",
    "train_loader, val_loader, test_loader = create_split_loaders(batch_size, seed, transform=transform, \n",
    "                                                             p_val=p_val, p_test=p_test,\n",
    "                                                             shuffle=True, show_sample=False, \n",
    "                                                             extras=extras)\n",
    "\n",
    "# Instantiate a densenet to run on the GPU or CPU based on CUDA support\n",
    "model = torchvision.models.densenet161(pretrained='imagenet')\n",
    "n_class = 14\n",
    "freeze = True\n",
    "if freeze:\n",
    "    for i, param in model.named_parameters():\n",
    "        param.requires_grad = False\n",
    "\n",
    "num_ftrs = model.classifier.in_features\n",
    "model.classifier = nn.Linear(num_ftrs, n_class)\n",
    "model = model.to(computing_device)\n",
    "\n",
    "print(\"Model on CUDA?\", next(model.parameters()).is_cuda)\n",
    "\n",
    "#TODO: Define the loss criterion and instantiate the gradient descent optimizer\n",
    "#criterion = torch.nn.MultiLabelSoftMarginLoss() #TODO - loss criteria are defined in the torch.nn package\n",
    "# criterion = torch.nn.CrossEntropyLoss() # 不知道选哪个\n",
    "criterion = MyBCEloss(computing_device, True, True)\n",
    "\n",
    "class Loss(torch.nn.Module):\n",
    "    def __init__(self, weight=None, pen=1e-1):\n",
    "        super(Loss, self).__init__()\n",
    "        self.weight = weight\n",
    "        self.pen = pen\n",
    "\n",
    "    def forward(self, y, t):\n",
    "        eps = 1e-8\n",
    "        diff = torch.abs(t-y)\n",
    "        c = -diff*(t*torch.log(y+eps)+self.pen*(1-t)*torch.log(1-y+eps))\n",
    "        if (self.weight is not None):\n",
    "            c *= self.weight\n",
    "\n",
    "        return torch.sum(c)\n",
    "WEIGHTS = torch.tensor([12.84306987, 55.5324418, 11.7501572, 7.83946301, 26.91956783, 24.54465849, 117.64952781, 30.0670421, 33.64945978, 67.95151515, 61.70610897, 91.7512275, 45.91318591, 671.37724551]).to(computing_device)/10\n",
    "criterion = Loss(weight=WEIGHTS, pen=0.4)\n",
    "\n",
    "#TODO: Instantiate the gradient descent optimizer - use Adam optimizer with default parameters\n",
    "optimizer = torch.optim.Adam(model.parameters(),lr = learning_rate) #TODO - optimizers are defined in the torch.optim package\n",
    "\n",
    "# trainer()\n",
    "# Track the loss across training\n",
    "total_loss = []\n",
    "avg_minibatch_loss = []\n",
    "validation_loss = []\n",
    "# Begin training procedure\n",
    "for epoch in range(num_epochs):\n",
    "    N = 50\n",
    "    N_minibatch_loss = 0.0  \n",
    "    N_acc=0.0\n",
    "    N_minibatch_acc = 0.0 \n",
    "    N_minibatch_precision = 0.0\n",
    "    N_minibatch_recall = 0.0 \n",
    "    N_minibatch_recall = 0.0\n",
    "    # Get the next minibatch of images, labels for training\n",
    "    for minibatch_count, (images, labels) in enumerate(train_loader, 0):\n",
    "\n",
    "        # Put the minibatch data in CUDA Tensors and run on the GPU if supported\n",
    "        images, labels = images.to(computing_device), labels.to(computing_device)\n",
    "        #print(images.requires_grad)\n",
    "        # Zero out the stored gradient (buffer) from the previous iteration\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Perform the forward pass through the network and compute the loss\n",
    "        outputs = model(images)\n",
    "        outputs = torch.sigmoid(outputs) \n",
    "        output =outputs.cpu().data.numpy()\n",
    "        label = labels.cpu().data.numpy()\n",
    "        output[output>=0.5]=1\n",
    "        output[output<0.5]=0\n",
    "        acc = np.sum(output==label)/(output.shape[0]*output.shape[1])\n",
    "        \n",
    "#         print(acc)\n",
    "\n",
    "#         assert 0==1\n",
    "        (accuracy,precision,recall,BCR),_ = evaluate(output,label)\n",
    "    \n",
    "        loss = criterion(outputs, labels)\n",
    "        # Automagically compute the gradients and backpropagate the loss through the network\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update the weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Add this iteration's loss to the total_loss\n",
    "        total_loss.append(loss.item())\n",
    "        N_minibatch_loss += loss\n",
    "        N_minibatch_acc += accuracy\n",
    "        N_acc+=acc\n",
    "        #TODO: Implement validation\n",
    "        if minibatch_count % N == 0:    \n",
    "            # Print the loss averaged over the last N mini-batches   \n",
    "            N_acc/=N\n",
    "            N_minibatch_loss /= N\n",
    "            N_minibatch_acc /= N\n",
    "            print('Epoch %d, average minibatch %d loss: %.3f acc:%.3f,ture acc %.3f' %\n",
    "                (epoch + 1, minibatch_count, N_minibatch_loss,N_minibatch_acc,N_acc ))\n",
    "            # Add the averaged loss over N minibatches and reset the counter\n",
    "            avg_minibatch_loss.append(N_minibatch_loss)\n",
    "            N_minibatch_loss = 0.0\n",
    "            N_minibatch_acc =0.0\n",
    "            N_acc=0.0\n",
    "            if minibatch_count %(600) ==0:\n",
    "                print('here we do validation')\n",
    "#                 temp_loss = test(model,val_loader)\n",
    "#                 validation_loss.append(temp_loss)\n",
    "                #TODO early stopping\n",
    "                \n",
    "    print(\"Finished\", epoch + 1, \"epochs of training\") \n",
    "print(\"Training complete after\", epoch+1, \"epochs\")\n",
    "print(\"Here we do test\")\n",
    "_=test(model,test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(output,label):\n",
    "    '''\n",
    "    params:\n",
    "        output:predicted logits.   shape:(minibatch_size,num_class)\n",
    "        label: ground label.       shape:(minibatch_size,num_class)\n",
    "    return:\n",
    "        (acc_list,precision_list,recall_list,BCR_list): accuracy,precision,recall and BCR per-class\n",
    "        (average_acc,average_precision,average_recall,average_BCR):averaged per-class accuracy,precision... with equal weight\n",
    "    '''\n",
    "    output[output>=0.5]=1\n",
    "    output[output<0.5]= 0\n",
    "    sample_size,class_size = output.shape\n",
    "    acc_list = np.zeros(class_size)\n",
    "    precision_list = np.zeros(class_size)\n",
    "    recall_list = np.zeros(class_size)\n",
    "    BCR_list =  np.zeros(class_size)\n",
    "    for class_id in range(len(acc_list)):\n",
    "        sub_output = output[:,class_id]\n",
    "        sub_label = label[:,class_id]\n",
    "        \n",
    "        acc_list[class_id] = (np.sum(sub_output==sub_label))/sample_size\n",
    "       \n",
    "        TP = np.sum(np.logical_and(sub_output == sub_label, sub_label == 1))\n",
    "        FP = np.sum((sub_output==1) == (sub_label==0))\n",
    "        FN = np.sum((sub_output==0) == (sub_label==1))\n",
    "        TN = np.sum(np.logical_and(sub_output == sub_label, sub_label == 0))\n",
    "#         print(np.sum(sub_output==sub_label),(TP+TN),(TP+TN+FN+FP))\n",
    "        acc_list[class_id] = (TP+TN)/sample_size\n",
    "        if TP ==0:\n",
    "            precision_list[class_id]=0\n",
    "            recall_list[class_id] =0\n",
    "        else:\n",
    "            precision_list[class_id] = TP/(TP+FP)\n",
    "            recall_list[class_id] = TP/(TP+FN)\n",
    "        BCR_list [class_id] =(precision_list[class_id]+recall_list[class_id])/2.0\n",
    "        \n",
    "    average_acc= np.mean(acc_list)\n",
    "    average_precision = np.mean(precision_list)\n",
    "    average_recall = np.mean(recall_list)\n",
    "    average_BCR = np.mean(BCR_list)\n",
    "    return (average_acc,average_precision,average_recall,average_BCR),(acc_list,precision_list,recall_list,BCR_list)\n",
    "\n",
    "evaluate(output,label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
